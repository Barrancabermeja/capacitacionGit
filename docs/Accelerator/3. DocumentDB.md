# DocumentDB; Storage and indexing of Arbitrary Data Structures

Microsoft Azure DocumentDB is the highly-scalable NoSQL document database-as-a-service that offers rich query and transactions over schema-free data, helps deliver reliable and predictable performance, and enables rapid development.

To highlight the capability of the schema free database we will show an architectural approach to address the schema free and embrace loose typings. This will be shown through storage of the product catalogue with products which contain deeply nested attributes and show an approach how they can be shown and queried across.

We also highlight common mistakes made when implementing and querying DocumentDB and ways in which to avoid or address these.

## Setup

Modifications to Parts Unlimited for this section will require a DocumentDB database account created in Azure and linked to your application. To house our products, we will create a database inside our DocumentDB Account called "PartsUnlimited", with a Collection called "ProductCollection". The config.json file found inside the root of the PartsUnlimited project contains a "DocumentDB" section which specifies the DocumentDB URI and Key to use unique to your instance of DocumentDB.
		
## Managing collections and items

### Schema free storage approach

Consider the Parts Unlimited scenario of building an e-commerce site where products can cover a wide range of items with unique and different properties. It soon becomes difficult to present and maintain this type of unstructured data in a tabular format. Creating a column for each possible attribute doesn't scale because there are too many varying attributes among your various products. Alternatively, creating a table for each product type is cumbersome given the potential variety of the product catalog. In addition to this, maintaining and updating columns of various products over time becomes a large maintenance cost and carries significant risk when the application relies on a strictly set schema. Storing products as JSON in a single varchar column has significant drawbacks in regards to performance, and you lose the ability to index and query against individual properties. This is where the use of schema free storage in the form of Azure DocumentDB should be considered. Storing heterogeneous data, or data with an undefined schema with the potential of changing frequently in a schema-agnostic database solves these issues while still providing full indexing, enabling you to easily and efficiently query against your data set.

Naturally, a schema will need to be applied to the data for use with the application at some stage, with common approaches doing so in either the application layer (e.g. deserializing result sets to a typed object), or within the UI layer for presentation to the end user. An alternative option is to store the schema against the item inside DocumentDB, and creating templates in your UI layer that can consume this schema at runtime in order to display data correctly. This makes it possible to still have a defined schema, which can be altered without need for changes to the application layer or UI layer itself, providing the flexibility of storing data in a schema agnostic database while still providing some form of a defined schema for use within your application.

### Collection partitioning strategy
	
Collections act as highly available data partitions for document storage and processing. A common practice when migrating from a SQL database implementation to DocumentDB is to define an individual DocumentDB collection for each table or item type. It is however important to consider that a collection within DocumentDB can store heterogeneous items with a diverse range of content and not tied to a particular type or schema.

Additionally, Azure DocumentDB pricing is based on a "per collection" model, with more collections incurring higher costs. It is therefore recommended to have a collection partitioning strategy that minimises the number of collections. You should not think of collections as SQL-style tables, but rather as partitions that provide boundaries for transactions and query execution. 

Your collection partitioning strategy should be driven by capacity (individual collections have a 10GB capacity constraint) or throughput requirements for each individual collection, as individual collections can be assigned different performance tiers (see the throttling section below).

Different "types" of items can be stored within an individual collection by creating a "type" delimiter property for each entry within the collection to enable filtering by item type. e.g.

```sql
SELECT * FROM c WHERE c.type = "Product"
```

It is worth noting that by default _all_ properties within DocumentDB are hash-indexed, resulting in a negligible performance hit for filtering for items in this manner. Note that if you opt to specify a custom indexing policy then be sure that the "type" property is also indexed to ensure maximum performance. More information on DocumentDB indexing policies can be found [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-indexing-policies/).


### Attachments

> Attachments is a preview feature

DocumentDB allows the storage of binary blobs either in your DocumentDB database, or externally on your own remote media store. Metadata about the Attachment is stored in DocumentDB.

When creating, updating or deleting attachments DocumentDB will take care of the minutia of creating, maintaining and cleaning up the associated blobs content.

You can read more about the support for Attachments in DocumentDB [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-resources/#attachments-and-media).


### Server-side partitioning

Server-side partitioning was introduced for DocumentDB in March 2016 with the release of version 1.6 of the `Microsoft.Azure.DocumentDB` NuGet package. Previously all partitioning logic had to be implemented manually on the client-side.

While your existing client-side partitioning code will continue to work, you can now offload that work to the server by taking advantage of the new APIs. To create a partitioned collection, provide a path to the property on which the partitions should be keyed when you define the collection. E.g.

```csharp
var deviceCollection = new DocumentCollection();
deviceCollection.Id = "devices";
deviceCollection.PartitionKey.Paths.Add("/customerId");

await client.CreateDocumentCollectionAsync(
	UriFactory.CreateDatabaseUri("devicedb"),
	deviceCollection,
	new RequestOptions {
		OfferThroughput = 20000
	});

```

> Note that `OfferThroughput` must be more than 10,000 to enable server-side partitioning.

New documents will be automatically added to the appropriate partition based on the partition key extracted from the document. For querying purposes however you must now provide a partition key. E.g.

```csharp
// Build the document URI in the normal fashion
var documentUri = UriFactory.CreateDocumentUri("devicedb", "devices", "23d5a962e76546899837ee5863488dcd");

// Also provide the appropriate partition key - in this case a customer identifier
var options = new RequestOptions { PartitionKey = new PartitionKey("112-fabrikam-llc") });

var document = await client.ReadDocumentAsync(documentUri, options);
```


## Querying DocumentDB
	
### Complex arbitrary JSON documents

Azure DocumentDB provides the ability to query documents using a subset of SQL as a schema free JSON query language. This is possible through the automatic indexing of JSON documents, foregoing the need for creating explicit schemas, or creating secondary indexes. This effectively means that all we need to do in order to query complex JSON documents, is to insert the document into a DocumentDB collection, and query the document using the SQL language provided. It is however important to note the differences between dealing with DocumentDB in comparison to a more traditional SQL database. DocumentDB SQL works with JSON, and therefore a tree shaped structure, as opposed to rows and columns, giving us the ability to refer to nodes in the tree by depth e.g. Node1.Node2.Node3 etc. Therefore, DocumentDB only works with [supported JSON](http://www.json.org/) formatted documents. While DocumentDB supports hierarchical, relational and spacial queries, along with standard ANSI-SQL keywords and operations, it is also worth noting some SQL equivalent operations are not supported for DocumentDB, with a comprehensive list available [here](https://msdn.microsoft.com/en-us/library/azure/dn782250.aspx).

DocumentDB enables us to perform fast, strongly typed queries on arbitrary data structures. We have the ability to query across deeply nested properties within a collection of JSON documents through the combination of the fully indexed nature of DocumentDB, as well as the fact that relations in data entities are implicitly captured by containment inside the document as opposed to primary/foreign key relations. This enables us to query deeply nested, complex JSON documents without the need for specifying a structure or schema to our query in advance. The query return type as a result, is bound dynamically, and therefore the same expression can yield different types when executed on different documents i.e. the result of a query is not guaranteed to be of a fixed schema.

Below is an example of a document representing a product. The product contains descriptive details with nested properties e.g. the `Brightness` property has nested values `Lumens` and `Watts`, with a string and numeric type respectively. 

```json
"Product": {
	"SkuNumber": "LIG-0001",
	"ProductId": 1,    
	"ProductDetailList": {
		"Light Source": "Halogen",
		"Assembly Required": "Yes",
		"Color": "Clear",
		"Interior": "Chrome",
		"Beam": "low and high",
		"Wiring harness included": "Yes",
		"Bulbs Included": "No",
		"Includes Parking Signal": "Yes",
		"Brightness": {
			"Lumens": 1200,
			"Watts": "80w"    
		}
	}
}
```
 
The indexing capabilities of DocumentDB enable us to effeciently query against these nested properties by 'drilling' down into the document in the following manner:
We demonstrate this within the Parts Unlimited solution by loading related products on the product detail page.
The implementation of this can be seen in the [RelatedProductsQueryBuilder](../../src/PartsUnlimitedWebsite/Repository/RelatedProductsQueryBuilder.cs) and specifically [LightingRelatedProductQueryStrategy](../../src/PartsUnlimitedWebsite/Repository/LightingRelatedProductQueryStrategy.cs) where we find related products ordered by the `Brightness.Lumens` attribute.

```csharp
public SqlQuerySpec BuildQuery(Product product)
{
	return new SqlQuerySpec("SELECT * " +
							"FROM products " +
							"ORDER BY products.ProductDetailList.Brightness.Lumens DESC");
}
```

Note the use of the `ORDER BY` operation against the numeric value of `Lumens`, illustrating that DocumentDB gives us the capability of using the full range of DocumentDB SQL operations available on these nested properties, and that DocumentDB is not simply making use of a 'FULLTEXT' index on deeply nested properties.

DocumentDB further enables us to ensure high performance when dealing with these types of queries by allowing us to specify custom indexing policies in order to granularly manage the trade-off between query performance, write performance and index storage overhead efficiency. While default indexing can prove suitable for a wide array of applications by providing the largest amount of out of the box flexibility with balanced trade-offs between performance and storage efficiency, it is possible to identify common query patterns within your application, and to optimise the DocumentDB indexing policy to better match these patterns. 

Automatic indexing can selectively be turned on or off, for when only a subset or certain type of document needs to be queried. Note that unindexed documents can still be accesses through their selflink, or ID property. Additionally, certain paths can be excluded on indexed documents as well, when a set of documents only require the availability of certain queries to be run against them. Both approaches will decrease the indexing overhead across the collection, while improving write performance on the specific documents. The index precision can be set for a subset of documents to reflect the required query performance for the specific subset. Higher index precision results in faster queries, but also incur a higher storage overhead. The index type itself can also be set between 'Hash' or 'Range'. Hash is sufficient for equality operations against a subset of documents, but it is important to note that in order to perform range or 'order by' queries, an indexing type of 'Range' must be set on the subset of documents.


### Case-insensitive querying

DocumentDB does not support case-insensitve string comparisons. There are two ways to work around this. You could use DocumentDB's built-in `UPPER` and `LOWER` functions:

```sql
SELECT * FROM book WHERE LOWER(book.Title) = LOWER('Introduction to DocumentDB')
```

This technique however will not take full advantage of the indexes on your data, which are case sensitive. Instead, a more efficient technique is to store a canonicalized form of the search field on your document and execute your queries against that. E.g.

```json
{
	"id": "23d5a962e76546899837ee5863488dcd",
	"Title": "Introduction to DocumentDB",
	"LowerCaseTitle": "introduction to documentdb",
	"Author": "Joe Bloggs"
}
```

You could then execute the following query, which will take advantage of the indexes on your collection:

```sql
SELECT * FROM book WHERE book.LowerCaseTitle = LOWER('Introduction to DocumentDB')
```

If you are mapping into C# classes, you can use the following technique to automatically generate the canonicalized fields:

```csharp
public class BookInfo
{
	public string Id { get; set; }
	public string Title { get; set; }
	public string Author { get; set; }
	
	// C# 6 read-only property expression syntax
	public string LowerCaseTitle => Title?.ToLower();
}
```


### Caching query objects

A common mistake is to create a new DocumentClient each time the application intends to perform a request to DocumentDB. Each DocumentClient instance is thread-safe and performs efficient connection management and address caching when operating in Direct Mode. Therefore, creating a new instance of DocumentClient is an expensive operation that can have performance implications. To allow efficient connection management and better performance by DocumentClient, it is recommended to use a single instance of DocumentClient per AppDomain for the lifetime of the application.

In Azure DocumentDB, each document has a system-generated SelfLink. These SelfLinks are guaranteed to be unique and immutable for the lifetime of the document, and reading a single document using a SelfLink is commonly referenced as the most efficient way to consume a single document. It is important to note however, that a common mistake is to create new instances of 'GetOrCreateDatabase' or 'GetOrCreateCollection' every time a reference to a database or collection is needed in order to retrieve SelfLinks. This results in multiple queries to DocumentDB for every single intended operation, and can result in exceeding your request quota and getting throttled. It is therefore recommended to cache these objects whenever suitable if they are required by the application.

It is currently possible to do away with SelfLinks to a large extent within your application, avoiding this issue entirely. An UriFactory can be used to construct links that are based on the ID property of items, and therefore a query for databases or collections is not needed in many cases. In the event where the application has to ensure that a collection or database exists, 'GetOrCreateDatabase' or 'GetOrCreateCollection' can still be used, but the return objects should then be cached to avoid the issues highlighted above.

To assist in creating links based on the ID property of documents, the `UriFactory` class found inside the DocumentDB SDK can be utilised in a manner below:

```csharp
public Uri BuildProductCollectionLink()
{
	return UriFactory.CreateDocumentCollectionUri(DatabaseId, CollectionId);
}

public Uri BuildProductLink(int productId)
{
	return UriFactory.CreateDocumentUri(DatabaseId, CollectionId, productId.ToString());
}

public Uri BuildAttachmentLink(int productId)
{
	return UriFactory.CreateAttachmentUri(DatabaseId, CollectionId, productId.ToString(), productId.ToString());
}

public Uri BuildDatabaseLink()
{
	return UriFactory.CreateDatabaseUri(DatabaseId);
}
```
	
A query can then be made against a collection without the need for querying for the database or collection first:

```csharp
public async Task<IEnumerable<IProduct>> LoadAllProducts()
{
	var collection = _configuration.BuildProductCollectionLink();
	return await _client.CreateDocumentQuery<Product>(collection)
		.ToAsyncEnumerable().ToList();
}
```
		
### Managing throttling

Azure DocumentDB implements a reserved throughput model for each individual collection that resides in your database account. Throughput requirements can be managed for individual collections as requirements from the application change by setting their respective performance levels, which can be viewed [here](https://azure.microsoft.com/en-us/documentation/articles/documentdb-performance-levels/)

As a result of throughput throttling on individual collections, it is possible to observe confusing behaviour from your application as certain requests to DocumentDB succeed and others fail once they have exceeded their individual request quota and get throttled. It is therefore important to account for this behaviour within your application.

When the application exceeds the allowed throughput for the collection, DocumentDB will pre-emptively end the request and respond with HTTP status code 429 (Request too large), and return a "x-ms-retry-after-ms" header representing the amount of time (in milliseconds) that the application must wait before attempting another request:

```http
HTTP Status: 429
Status Line: RequestRateTooLarge
x-ms-retry-after-ms: 100
```
	
When using the DocumentDB .Net SDK with LINQ, the SDK will automatically retry the failed operation internally when it encounters an HTTP 429. There are however scenarios where default throttling exception behaviour from the SDK may not be sufficient, and in such cases the application can be modified to handle the RequestRateTooLargeException. A utility function allowing for throttling and retrying a task when the appropriate waiting time has been satisfied can be implemented as follows:

```csharp
public static async Task<V> ExecuteTaskWithThrottlingSafety<V>(Func<Task<V>> func)
{
	while (true)
	{
		try
		{
			return await func();
		}
		catch (AggregateException ae) when (ae.InnerException is DocumentClientException)
		{
			var de = (DocumentClientException)ae.InnerException;
			if (de.StatusCode != null && (int)de.StatusCode == 429)
			{
				await Task.Delay(de.RetryAfter);
			}
			else
			{	
				throw;
			}                   
		}
	}
}
```
	
This can then be used to call an operation against DocumentDB while allowing for throttling safety and retry functionality:

```csharp
Product newProduct = new Product();
ResourceResponse<Document> response = await ExecuteTaskWithThrottlingSafety(client, () => client.CreateDocumentAsync(collectionSelfLink, newProduct));
```

### Result sets as JSON to client

There is a performance overhead involved with serialising and deserialising JSON documents when documents are retrieved from DocumentDB. It is however currently possible to skip this overhead and pass the JSON document directly to the client. The deserialisation process is triggered the moment a property on the retrieved document is accessed e.g.

```csharp
var id = document.Id;
```
	
This can be circumvented by using a JsonWriter on the document object in order to retrieve the string representation of the document. With this method, the deserialization process is skipped. Additionally it is possible to load from the string directly. Resource.LoadFrom is a method available inside the DocumentDB SDK, which enables loading from a specified JSON reader. More information can be found [here](https://msdn.microsoft.com/en-us/library/azure/microsoft.azure.documents.resource.aspx).
